{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f5f13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install librosa if not yet installed\n",
    "# !pip install librosa\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c84fabb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "dataset_path = os.path.join(os.getcwd(),'..\\cmu-mosi\\Custom\\Audio')\n",
    "\n",
    "data_array = []\n",
    "for name in glob.glob(dataset_path+'\\*\\*'):\n",
    "    \n",
    "    label = os.path.basename(os.path.dirname(name))\n",
    "    \n",
    "    data_array.append([\n",
    "        name,\n",
    "        label\n",
    "    ])\n",
    "    \n",
    "df = pd.DataFrame(data_array, columns=['path','label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1048416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required modules\n",
    "# !pip install resampy\n",
    "# !pip install tf-slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288c134e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/tensorflow/models.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a957706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -O https://storage.googleapis.com/audioset/vggish_model.ckpt\n",
    "# !curl -O https://storage.googleapis.com/audioset/vggish_pca_params.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8faa290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the source files to the current directory.\n",
    "# !cp models/research/audioset/vggish/* .\n",
    "# !xcopy .\\models\\research\\audioset\\vggish\\ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f102e8a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run smoke test\n",
    "# from vggish_smoke_test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dfb7041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vggish_slim\n",
    "import vggish_params\n",
    "import vggish_input\n",
    "import tensorflow.compat.v1 as tf\n",
    "import vggish_postprocess\n",
    "import time\n",
    "def CreateVGGishNetwork(hop_size=0.96):   # Hop size is in seconds.\n",
    "  ###\n",
    "  # Define VGGish model, load the checkpoint, and return a dictionary that points\n",
    "  # to the different tensors defined by the model.\n",
    "  ###\n",
    "  vggish_slim.define_vggish_slim()\n",
    "  checkpoint_path = 'vggish_model.ckpt'\n",
    "  vggish_params.EXAMPLE_HOP_SECONDS = hop_size\n",
    "  vggish_slim.load_vggish_slim_checkpoint(sess, checkpoint_path)\n",
    "  features_tensor = sess.graph.get_tensor_by_name(\n",
    "      vggish_params.INPUT_TENSOR_NAME)\n",
    "  embedding_tensor = sess.graph.get_tensor_by_name(\n",
    "      vggish_params.OUTPUT_TENSOR_NAME)\n",
    "  layers = {'conv1': 'vggish/conv1/Relu',\n",
    "            'pool1': 'vggish/pool1/MaxPool',\n",
    "            'conv2': 'vggish/conv2/Relu',\n",
    "            'pool2': 'vggish/pool2/MaxPool',\n",
    "            'conv3': 'vggish/conv3/conv3_2/Relu',\n",
    "            'pool3': 'vggish/pool3/MaxPool',\n",
    "            'conv4': 'vggish/conv4/conv4_2/Relu',\n",
    "            'pool4': 'vggish/pool4/MaxPool',\n",
    "            'fc1': 'vggish/fc1/fc1_2/Relu',\n",
    "#             'fc2': 'vggish/fc2/Relu',\n",
    "            'embedding': 'vggish/embedding',\n",
    "            'features': 'vggish/input_features',\n",
    "         }\n",
    "  g = tf.get_default_graph()\n",
    "  for k in layers:\n",
    "    layers[k] = g.get_tensor_by_name( layers[k] + ':0')\n",
    "  return {'features': features_tensor,\n",
    "          'embedding': embedding_tensor,\n",
    "          'layers': layers,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15e5141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessWithVGGish(vgg, x, sr):\n",
    "  '''Run the VGGish model, starting with a sound (x) at sample rate\n",
    "  (sr). Return a whitened version of the embeddings. Sound must be scaled to be\n",
    "  floats between -1 and +1.'''\n",
    "  # Produce a batch of log mel spectrogram examples.\n",
    "  input_batch = vggish_input.waveform_to_examples(x, sr)\n",
    "  # print('Log Mel Spectrogram example: ', input_batch[0])\n",
    "  [embedding_batch] = sess.run([vgg['embedding']],\n",
    "                               feed_dict={vgg['features']: input_batch})\n",
    "  # Postprocess the results to produce whitened quantized embeddings.\n",
    "  pca_params_path = 'vggish_pca_params.npz'\n",
    "  pproc = vggish_postprocess.Postprocessor(pca_params_path)\n",
    "  postprocessed_batch = pproc.postprocess(embedding_batch)\n",
    "  # print('Postprocessed VGGish embedding: ', postprocessed_batch[0])\n",
    "  return postprocessed_batch[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "175157f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\tensorflow_and_keras\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "C:\\Users\\User\\anaconda3\\envs\\tensorflow_and_keras\\lib\\site-packages\\tensorflow\\python\\keras\\legacy_tf_layers\\core.py:336: UserWarning: `tf.layers.flatten` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Flatten` instead.\n",
      "  warnings.warn('`tf.layers.flatten` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 minutes has passed, index is 0\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\negative\\0h-zjBukYpk_12.wav\n",
      "2.7666666666666666 minutes has passed, index is 100\n",
      "5.45 minutes has passed, index is 200\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\negative\\BvYR0L6f2Ig_22.wav\n",
      "7.683333333333334 minutes has passed, index is 300\n",
      "10.266666666666667 minutes has passed, index is 400\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\negative\\d6hH302o4v8_16.wav\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\negative\\d6hH302o4v8_40.wav\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\negative\\d6hH302o4v8_5.wav\n",
      "12.716666666666667 minutes has passed, index is 500\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\negative\\k5Y_838nuGo_23.wav\n",
      "14.9 minutes has passed, index is 600\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\negative\\lXPQBPVc5Cw_11.wav\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\negative\\OQvJTdtJ2H4_1.wav\n",
      "17.883333333333333 minutes has passed, index is 700\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\negative\\OtBXNcAL_lE_18.wav\n",
      "20.733333333333334 minutes has passed, index is 800\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\negative\\VbQk4H8hgr0_11.wav\n",
      "23.8 minutes has passed, index is 900\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\negative\\VbQk4H8hgr0_8.wav\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\negative\\VCslbP0mgZI_1.wav\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\negative\\W8NXH0Djyww_2.wav\n",
      "26.75 minutes has passed, index is 1000\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\neutral\\03bSnISJMiM_5.wav\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\neutral\\9qR7uwkblbs_33.wav\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\neutral\\BI97DNYfe5I_20.wav\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\neutral\\BvYR0L6f2Ig_18.wav\n",
      "28.916666666666668 minutes has passed, index is 1100\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\neutral\\v0zCBqDeKcE_16.wav\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\positive\\2WGyTLYerpo_2.wav\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\positive\\2WGyTLYerpo_21.wav\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\positive\\2WGyTLYerpo_25.wav\n",
      "30.75 minutes has passed, index is 1200\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\positive\\5W7Z1C_fDaE_4.wav\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\positive\\73jzhE8R1TQ_4.wav\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\positive\\7JsX8y1ysxY_10.wav\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\positive\\7JsX8y1ysxY_9.wav\n",
      "32.833333333333336 minutes has passed, index is 1300\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\positive\\9J25DZhivz8_15.wav\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\positive\\aiEXnCPZubE_28.wav\n",
      "34.88333333333333 minutes has passed, index is 1400\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\positive\\BI97DNYfe5I_21.wav\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\positive\\bOL9jKpeJRs_14.wav\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\positive\\bOL9jKpeJRs_30.wav\n",
      "36.88333333333333 minutes has passed, index is 1500\n",
      "39.28333333333333 minutes has passed, index is 1600\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\positive\\cXypl4FnoZo_23.wav\n",
      "41.4 minutes has passed, index is 1700\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\positive\\I5y0__X72p0_15.wav\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\positive\\I5y0__X72p0_31.wav\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\positive\\I5y0__X72p0_32.wav\n",
      "42.983333333333334 minutes has passed, index is 1800\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\positive\\Jkswaaud0hk_20.wav\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\positive\\LSi-o-IrDMs_24.wav\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\positive\\LSi-o-IrDMs_8.wav\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\positive\\Njd1F0vZSm4_5.wav\n",
      "44.56666666666667 minutes has passed, index is 1900\n",
      "FAILED:  C:\\Users\\User\\Documents\\School\\Term 6\\CDS\\Project\\speech-emotion\\..\\cmu-mosi\\Custom\\Audio\\positive\\tStelxIAHjw_16.wav\n",
      "46.733333333333334 minutes has passed, index is 2000\n",
      "48.63333333333333 minutes has passed, index is 2100\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "# now we extract features using VGGish, and feed it back to our custom neural network layer\n",
    "# data_frame =pd.DataFrame({\"lists\": list(audio_arr)},columns=['lists'])\n",
    "vgg = CreateVGGishNetwork(0.01)\n",
    "\n",
    "# load audio files\n",
    "audio_arr = []\n",
    "\n",
    "start = time.time()\n",
    "count = 1\n",
    "for index, row in df.iterrows():\n",
    "    if index % 100 == 0:\n",
    "        print(\"{} minutes has passed, index is {}\".format(int(time.time() - start)/60, index))\n",
    "        count+=1\n",
    "    try:\n",
    "        y, sr = librosa.load(row['path'], sr=44100)\n",
    "        data = ProcessWithVGGish(vgg,y,44100)\n",
    "        audio_arr.append(data)\n",
    "    except:\n",
    "        print('FAILED: ',row['path'])\n",
    "        df.drop(index)\n",
    "    \n",
    "\n",
    "# data = FeaturesFromVGGish(vgg,data_frame.iloc[0]['lists'],44100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c495efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "x = keras.preprocessing.sequence.pad_sequences(\n",
    "    audio_arr, padding=\"post\"\n",
    ")\n",
    "le = LabelEncoder()\n",
    "y = to_categorical(le.fit_transform(np.array(df['label'].tolist())))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75353aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(3))\n",
    "model.add(layers.Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a8749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "num_batch_size = 32\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "model.fit(x_train, y_train, epochs=num_epochs, batch_size=num_batch_size, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37a8082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: {0:.2%}\".format(score[1]))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: {0:.2%}\".format(score[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5369cf35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
